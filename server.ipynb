{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855fbaf1-e265-4ffa-b1ec-bd89f885b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "import sys\n",
    "import nest_asyncio\n",
    "from werkzeug.serving import run_simple\n",
    "from flask_cors import CORS\n",
    "from fastai.tabular.all import df_shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38d66d0-bc38-40e3-b453-71423761ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_constant(file):\n",
    "    \n",
    "    print(f\"Loading file: {file}...\")\n",
    "    with open(f\"commons/{file}\", \"r\") as f:\n",
    "        features = json.load(f)\n",
    "        \n",
    "    return features\n",
    "    \n",
    "def pre_process(file, df):\n",
    "    print(f\"Processing file: {file}...\")\n",
    "    print(f\"\\tDimensions before process: {df.shape}\")\n",
    "    \n",
    "    print(f\"\\tStrip columns name...\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    print(f\"\\tRename columns...\")\n",
    "    df.rename(columns=mapper_features, inplace=True)\n",
    "\n",
    "    # backup origin dataframe\n",
    "    origin_df = df.copy()\n",
    "    \n",
    "    print(f\"\\tDrop columns...\")\n",
    "    df.drop(columns=drop_features, inplace=True)\n",
    "    \n",
    "    print(f\"\\tReplace 'infinity value' by 'nan'...\")\n",
    "    df.replace(to_replace=[np.inf, -np.inf], value=np.nan, inplace=True)\n",
    "    \n",
    "    # print(f\"\\tDrop rows having 'nan' value...\")\n",
    "    # print(f\"\\t...has been droping {df.isna().any(axis=1).sum()} rows\")\n",
    "    # df.dropna(inplace=True)\n",
    "    \n",
    "    # print(f\"\\tDrop duplicate rows...\")\n",
    "    # print(f\"\\t...has been droping {df.duplicated().sum()} rows...\")\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # print(f\"\\tReset index...\")\n",
    "    # df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print(f\"\\tShrink data-frame type...\")\n",
    "    df = df_shrink(df)\n",
    "    print(f\"\\tDimensions after process: {df.shape}\")\n",
    "\n",
    "    return df, origin_df\n",
    "\n",
    "def predict_process(origin_df, df, file, model):\n",
    "    print(f\"Dataframe belong to file: {file}\")\n",
    "    print(f\"Start predicting...\")\n",
    "    X = df.drop(columns='Label')\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    arr = np.array(y_pred)\n",
    "    unique_values, counts = np.unique(arr, return_counts=True)\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        print(f'{value}: {count}')\n",
    "\n",
    "    print(f\"Saving result...\")\n",
    "    origin_df['Label'] = y_pred\n",
    "    origin_df.to_csv(f\"predict_result/{file.split('\\\\')[-1]}\", index=False)\n",
    "\n",
    "    result = df.to_dict(orient='records')\n",
    "    with open(f\"commons/data/{file.split('\\\\')[-1].replace('.csv', '.json')}\", 'w') as json_file:\n",
    "        json.dump(result, json_file, indent=4)\n",
    "    \n",
    "    return origin_df\n",
    "\n",
    "def load_model(file):\n",
    "    print(f\"Loading model from {file}...\")\n",
    "    model = joblib.load(file, mmap_mode='r')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10532725-f580-48f8-b74e-46cb6955dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: mapper_for_predicting.json...\n",
      "Loading file: drop_for_predicting.json...\n",
      "Loading model from modelset/model-100-20.joblib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "mapper_features = load_constant(\"mapper_for_predicting.json\")\n",
    "drop_features = load_constant(\"drop_for_predicting.json\")\n",
    "model = load_model('modelset/model-100-20.joblib')\n",
    "\n",
    "@app.route('/api/files', methods=['GET'])\n",
    "def list_files():\n",
    "    try:\n",
    "        files = glob.glob('share/*.csv')\n",
    "        return jsonify({\"files\": files}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "@app.route('/api/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.json\n",
    "        file_path = data.get('filepath')\n",
    "        print(file_path)\n",
    "        file = glob.glob(f\"{file_path}\")[0]\n",
    "        print(file)\n",
    "        df, origin_df = pre_process(file, pd.read_csv(file))\n",
    "        DATA = predict_process(origin_df, df, file, model)\n",
    "\n",
    "        return jsonify({'message':'successfully'})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/api/result', methods=['POST'])\n",
    "def read_excel_file():\n",
    "    try:\n",
    "        req_body = request.get_json()\n",
    "        file_path = req_body.get('filepath')\n",
    "        page = int(req_body.get('page', 1))\n",
    "        limit = int(req_body.get('limit', 10))\n",
    "        feature_name = req_body.get('featureName')\n",
    "        feature_value = req_body.get('featureValue')\n",
    "        label = req_body.get('label')\n",
    "\n",
    "        df = pd.read_csv(f\"predict_result/{file_path.split('\\\\')[-1]}\")\n",
    "        \n",
    "        filtered_df = df\n",
    "        if feature_name and feature_value:\n",
    "            if feature_name in df.columns:\n",
    "                filtered_df = filtered_df[filtered_df[feature_name].astype(str).str.contains(feature_value, case=False, na=False)]\n",
    "            else:\n",
    "                return jsonify({'error': f\"Feature '{feature_name}' not found in data\"}), 400\n",
    "\n",
    "        if label:\n",
    "            filtered_df = filtered_df[filtered_df['Label'] == label]\n",
    "    \n",
    "        # Pagination\n",
    "        total_items = len(filtered_df)\n",
    "        start = (page - 1) * limit\n",
    "        end = start + limit\n",
    "        paginated_df = filtered_df[start:end]\n",
    "\n",
    "        print(f\"Start: {start}\")\n",
    "        print(f\"End: {end}\")\n",
    "        # print(paginated_df)\n",
    "        result = paginated_df.to_dict(orient='records')\n",
    "    \n",
    "        # Return JSON response\n",
    "        return jsonify({\n",
    "            'page': page,\n",
    "            'limit': limit,\n",
    "            'totalItems': total_items,\n",
    "            'totalPages': (total_items + limit - 1) // limit,\n",
    "            'data': result\n",
    "        })\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/api/statistic', methods=['POST'])\n",
    "def get_counts():\n",
    "    try:\n",
    "        req_body = request.get_json()\n",
    "        file_path = req_body.get('filepath')\n",
    "        counts = {}\n",
    "        features = ['Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol', 'Label', 'Timestamp']\n",
    "        df = pd.read_csv(f\"predict_result/{file_path.split('\\\\')[-1]}\")\n",
    "        \n",
    "        for feature in features:\n",
    "            counts[feature] = df[feature].value_counts().to_dict()\n",
    "            \n",
    "        return jsonify(counts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "run_simple('localhost', 5000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054c9e5-11fc-4a23-95db-29c8ab648273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
